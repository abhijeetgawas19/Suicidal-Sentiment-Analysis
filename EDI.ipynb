{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import collections \n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import sklearn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Tweets_Mix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@juliaroy you are just a tumbling fool. Love it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>â youâ€™re stuck with me now iâ€™m not leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>i wake up and just want to sleep forever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@dotmariusz I'm a late bird  Mariusz - you fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>our friendly purge there is not a better way t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>this what you a saying many thieves and looter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                             Tweets\n",
       "0          1  you dont have to be crazy to work here serious...\n",
       "1          0  Paw pawing my ass off smh...im starting to fee...\n",
       "2          1  and ppl better not act like threatening suicid...\n",
       "3          0  @juliaroy you are just a tumbling fool. Love it. \n",
       "4          0  Just gonna go shopping up Fosse Park with mate...\n",
       "5          1   â youâ€™re stuck with me now iâ€™m not leav...\n",
       "6          1           i wake up and just want to sleep forever\n",
       "7          0  @dotmariusz I'm a late bird  Mariusz - you fro...\n",
       "8          1  our friendly purge there is not a better way t...\n",
       "9          1  this what you a saying many thieves and looter..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      "Sentiment    10000 non-null int64\n",
      "Tweets       10000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 5000, 0: 5000})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(data['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Function to remove the mentions  URL's  and String with @\n",
    "def removeURL(text):\n",
    "    tweet_out = re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "    re.sub('https?://[A-zA-z0-9]+','',text)\n",
    "    return tweet_out\n",
    "\n",
    "# Writing function to remove the non-numeric characters\n",
    "def removeNonAlphanumeric(text):\n",
    "    text_out = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tweet_No_URL\"]  = data[\"Tweets\"].apply(lambda x:removeURL(x))\n",
    "data[\"Tweet_No_Punc\"] = data[\"Tweets\"].apply(lambda x:removeNonAlphanumeric(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet_No_URL</th>\n",
       "      <th>Tweet_No_Punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smhim starting to feel b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@juliaroy you are just a tumbling fool. Love it.</td>\n",
       "      <td>you are just a tumbling fool. Love it.</td>\n",
       "      <td>juliaroy you are just a tumbling fool Love it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                             Tweets  \\\n",
       "0          1  you dont have to be crazy to work here serious...   \n",
       "1          0  Paw pawing my ass off smh...im starting to fee...   \n",
       "2          1  and ppl better not act like threatening suicid...   \n",
       "3          0  @juliaroy you are just a tumbling fool. Love it.    \n",
       "4          0  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                        Tweet_No_URL  \\\n",
       "0  you dont have to be crazy to work here serious...   \n",
       "1  Paw pawing my ass off smh...im starting to fee...   \n",
       "2  and ppl better not act like threatening suicid...   \n",
       "3            you are just a tumbling fool. Love it.    \n",
       "4  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                       Tweet_No_Punc  \n",
       "0  you dont have to be crazy to work here serious...  \n",
       "1  Paw pawing my ass off smhim starting to feel b...  \n",
       "2  and ppl better not act like threatening suicid...  \n",
       "3     juliaroy you are just a tumbling fool Love it   \n",
       "4  Just gonna go shopping up Fosse Park with mate...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    token = re.split('\\W+',text)\n",
    "    return token\n",
    "\n",
    "data [\"Tweet_Tokens\"] = data[\"Tweet_No_Punc\"].apply(lambda x:tokenization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet_No_URL</th>\n",
       "      <th>Tweet_No_Punc</th>\n",
       "      <th>Tweet_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>[you, dont, have, to, be, crazy, to, work, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smhim starting to feel b...</td>\n",
       "      <td>[Paw, pawing, my, ass, off, smhim, starting, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threatening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@juliaroy you are just a tumbling fool. Love it.</td>\n",
       "      <td>you are just a tumbling fool. Love it.</td>\n",
       "      <td>juliaroy you are just a tumbling fool Love it</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbling, fool, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>[Just, gonna, go, shopping, up, Fosse, Park, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>0</td>\n",
       "      <td>@valxx http://twitpic.com/2w1uj - And the artw...</td>\n",
       "      <td>http://twitpic.com/2w1uj - And the artwork's ...</td>\n",
       "      <td>valxx httptwitpiccom2w1uj  And the artworks by...</td>\n",
       "      <td>[valxx, httptwitpiccom2w1uj, And, the, artwork...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>1</td>\n",
       "      <td>i just love reading that im worthless \\r\\n\\r\\...</td>\n",
       "      <td>i just love reading that im worthless \\r\\n\\r\\...</td>\n",
       "      <td>i just love reading that im worthless \\r\\n\\r\\...</td>\n",
       "      <td>[, i, just, love, reading, that, im, worthless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>0</td>\n",
       "      <td>Disneyland was great! ... Got to go and pick ...</td>\n",
       "      <td>Disneyland was great! ... Got to go and pick ...</td>\n",
       "      <td>Disneyland was great  Got to go and pick up t...</td>\n",
       "      <td>[, Disneyland, was, great, Got, to, go, and, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>1</td>\n",
       "      <td>i hope i die in my sleep</td>\n",
       "      <td>i hope i die in my sleep</td>\n",
       "      <td>i hope i die in my sleep</td>\n",
       "      <td>[i, hope, i, die, in, my, sleep]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>i dont want to live anymore</td>\n",
       "      <td>i dont want to live anymore</td>\n",
       "      <td>i dont want to live anymore</td>\n",
       "      <td>[i, dont, want, to, live, anymore]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                             Tweets  \\\n",
       "0             1  you dont have to be crazy to work here serious...   \n",
       "1             0  Paw pawing my ass off smh...im starting to fee...   \n",
       "2             1  and ppl better not act like threatening suicid...   \n",
       "3             0  @juliaroy you are just a tumbling fool. Love it.    \n",
       "4             0  Just gonna go shopping up Fosse Park with mate...   \n",
       "...         ...                                                ...   \n",
       "9995          0  @valxx http://twitpic.com/2w1uj - And the artw...   \n",
       "9996          1   i just love reading that im worthless \\r\\n\\r\\...   \n",
       "9997          0   Disneyland was great! ... Got to go and pick ...   \n",
       "9998          1                           i hope i die in my sleep   \n",
       "9999          0                        i dont want to live anymore   \n",
       "\n",
       "                                           Tweet_No_URL  \\\n",
       "0     you dont have to be crazy to work here serious...   \n",
       "1     Paw pawing my ass off smh...im starting to fee...   \n",
       "2     and ppl better not act like threatening suicid...   \n",
       "3               you are just a tumbling fool. Love it.    \n",
       "4     Just gonna go shopping up Fosse Park with mate...   \n",
       "...                                                 ...   \n",
       "9995   http://twitpic.com/2w1uj - And the artwork's ...   \n",
       "9996   i just love reading that im worthless \\r\\n\\r\\...   \n",
       "9997   Disneyland was great! ... Got to go and pick ...   \n",
       "9998                           i hope i die in my sleep   \n",
       "9999                        i dont want to live anymore   \n",
       "\n",
       "                                          Tweet_No_Punc  \\\n",
       "0     you dont have to be crazy to work here serious...   \n",
       "1     Paw pawing my ass off smhim starting to feel b...   \n",
       "2     and ppl better not act like threatening suicid...   \n",
       "3        juliaroy you are just a tumbling fool Love it    \n",
       "4     Just gonna go shopping up Fosse Park with mate...   \n",
       "...                                                 ...   \n",
       "9995  valxx httptwitpiccom2w1uj  And the artworks by...   \n",
       "9996   i just love reading that im worthless \\r\\n\\r\\...   \n",
       "9997   Disneyland was great  Got to go and pick up t...   \n",
       "9998                           i hope i die in my sleep   \n",
       "9999                        i dont want to live anymore   \n",
       "\n",
       "                                           Tweet_Tokens  \n",
       "0     [you, dont, have, to, be, crazy, to, work, her...  \n",
       "1     [Paw, pawing, my, ass, off, smhim, starting, t...  \n",
       "2     [and, ppl, better, not, act, like, threatening...  \n",
       "3     [juliaroy, you, are, just, a, tumbling, fool, ...  \n",
       "4     [Just, gonna, go, shopping, up, Fosse, Park, w...  \n",
       "...                                                 ...  \n",
       "9995  [valxx, httptwitpiccom2w1uj, And, the, artwork...  \n",
       "9996  [, i, just, love, reading, that, im, worthless...  \n",
       "9997  [, Disneyland, was, great, Got, to, go, and, p...  \n",
       "9998                   [i, hope, i, die, in, my, sleep]  \n",
       "9999                 [i, dont, want, to, live, anymore]  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming (text):\n",
    "    out_text = [ps.stem(word) for word in text]\n",
    "    return out_text\n",
    "\n",
    "data['FirstDataSetStem'] = data['Tweet_Tokens'].apply(lambda x:stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [you, dont, have, to, be, crazi, to, work, her...\n",
       "1    [paw, paw, my, ass, off, smhim, start, to, fee...\n",
       "2    [and, ppl, better, not, act, like, threaten, s...\n",
       "3    [juliaroy, you, are, just, a, tumbl, fool, lov...\n",
       "4    [just, gonna, go, shop, up, foss, park, with, ...\n",
       "Name: FirstDataSetStem, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['FirstDataSetStem'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [you, dont, have, to, be, crazy, to, work, her...\n",
       "1    [Paw, pawing, my, as, off, smhim, starting, to...\n",
       "2    [and, ppl, better, not, act, like, threatening...\n",
       "3    [juliaroy, you, are, just, a, tumbling, fool, ...\n",
       "4    [Just, gonna, go, shopping, up, Fosse, Park, w...\n",
       "Name: FirstDataSet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    out_text = [wn.lemmatize(word) for word in text]\n",
    "    return out_text\n",
    "\n",
    "data['FirstDataSet'] =data['Tweet_Tokens'].apply(lambda x:lemmatize(x))\n",
    "\n",
    "data['FirstDataSet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet_No_URL</th>\n",
       "      <th>Tweet_No_Punc</th>\n",
       "      <th>Tweet_Tokens</th>\n",
       "      <th>FirstDataSetStem</th>\n",
       "      <th>FirstDataSet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>[you, dont, have, to, be, crazy, to, work, her...</td>\n",
       "      <td>[you, dont, have, to, be, crazi, to, work, her...</td>\n",
       "      <td>[you, dont, have, to, be, crazy, to, work, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smhim starting to feel b...</td>\n",
       "      <td>[Paw, pawing, my, ass, off, smhim, starting, t...</td>\n",
       "      <td>[paw, paw, my, ass, off, smhim, start, to, fee...</td>\n",
       "      <td>[Paw, pawing, my, as, off, smhim, starting, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threatening...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threaten, s...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threatening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@juliaroy you are just a tumbling fool. Love it.</td>\n",
       "      <td>you are just a tumbling fool. Love it.</td>\n",
       "      <td>juliaroy you are just a tumbling fool Love it</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbling, fool, ...</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbl, fool, lov...</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbling, fool, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>[Just, gonna, go, shopping, up, Fosse, Park, w...</td>\n",
       "      <td>[just, gonna, go, shop, up, foss, park, with, ...</td>\n",
       "      <td>[Just, gonna, go, shopping, up, Fosse, Park, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                             Tweets  \\\n",
       "0          1  you dont have to be crazy to work here serious...   \n",
       "1          0  Paw pawing my ass off smh...im starting to fee...   \n",
       "2          1  and ppl better not act like threatening suicid...   \n",
       "3          0  @juliaroy you are just a tumbling fool. Love it.    \n",
       "4          0  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                        Tweet_No_URL  \\\n",
       "0  you dont have to be crazy to work here serious...   \n",
       "1  Paw pawing my ass off smh...im starting to fee...   \n",
       "2  and ppl better not act like threatening suicid...   \n",
       "3            you are just a tumbling fool. Love it.    \n",
       "4  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                       Tweet_No_Punc  \\\n",
       "0  you dont have to be crazy to work here serious...   \n",
       "1  Paw pawing my ass off smhim starting to feel b...   \n",
       "2  and ppl better not act like threatening suicid...   \n",
       "3     juliaroy you are just a tumbling fool Love it    \n",
       "4  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                        Tweet_Tokens  \\\n",
       "0  [you, dont, have, to, be, crazy, to, work, her...   \n",
       "1  [Paw, pawing, my, ass, off, smhim, starting, t...   \n",
       "2  [and, ppl, better, not, act, like, threatening...   \n",
       "3  [juliaroy, you, are, just, a, tumbling, fool, ...   \n",
       "4  [Just, gonna, go, shopping, up, Fosse, Park, w...   \n",
       "\n",
       "                                    FirstDataSetStem  \\\n",
       "0  [you, dont, have, to, be, crazi, to, work, her...   \n",
       "1  [paw, paw, my, ass, off, smhim, start, to, fee...   \n",
       "2  [and, ppl, better, not, act, like, threaten, s...   \n",
       "3  [juliaroy, you, are, just, a, tumbl, fool, lov...   \n",
       "4  [just, gonna, go, shop, up, foss, park, with, ...   \n",
       "\n",
       "                                        FirstDataSet  \n",
       "0  [you, dont, have, to, be, crazy, to, work, her...  \n",
       "1  [Paw, pawing, my, as, off, smhim, starting, to...  \n",
       "2  [and, ppl, better, not, act, like, threatening...  \n",
       "3  [juliaroy, you, are, just, a, tumbling, fool, ...  \n",
       "4  [Just, gonna, go, shopping, up, Fosse, Park, w...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words after Stemming:17863\n",
      "Unique Words after Lemmatization:22399\n"
     ]
    }
   ],
   "source": [
    "listofwordsLemma = list(itertools.chain.from_iterable(data['FirstDataSet']))\n",
    "listofwordsStem = list(itertools.chain.from_iterable(data['FirstDataSetStem']))\n",
    "\n",
    "# Number of Unique Tokens in first Dataset\n",
    "\n",
    "print('Unique Words after Stemming:'+str(len(set(listofwordsStem))))\n",
    "print('Unique Words after Lemmatization:'+str(len(set(listofwordsLemma))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopWords(token_list):\n",
    "    text_out = [word for word in token_list if word not in stopwords]\n",
    "    return text_out\n",
    "\n",
    "data['SecondDataSet'] = data['FirstDataSetStem'].apply(lambda x:remove_stopWords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [dont, crazi, work, serious, illeg, requir, do...\n",
       "1           [paw, paw, ass, smhim, start, feel, better, ]\n",
       "2       [ppl, better, act, like, threaten, suicid, res...\n",
       "3                         [juliaroy, tumbl, fool, love, ]\n",
       "4       [gonna, go, shop, foss, park, mate, got, ï, ½7...\n",
       "                              ...                        \n",
       "9995    [valxx, httptwitpiccom2w1uj, artwork, billi, c...\n",
       "9996                 [, love, read, im, worthless, thank]\n",
       "9997    [, disneyland, wa, great, got, go, pick, kitti...\n",
       "9998                                   [hope, die, sleep]\n",
       "9999                           [dont, want, live, anymor]\n",
       "Name: SecondDataSet, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SecondDataSet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words After Stop Words Removed:17752\n"
     ]
    }
   ],
   "source": [
    "# Unique Words After Remove Stop Words \n",
    "\n",
    "SecondDataSet = list(itertools.chain.from_iterable(data['SecondDataSet']))\n",
    "\n",
    "print(\"Unique Words After Stop Words Removed:\"+str(len(set(SecondDataSet))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Words with Fewer Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet_No_URL</th>\n",
       "      <th>Tweet_No_Punc</th>\n",
       "      <th>Tweet_Tokens</th>\n",
       "      <th>FirstDataSetStem</th>\n",
       "      <th>FirstDataSet</th>\n",
       "      <th>SecondDataSet</th>\n",
       "      <th>ThirdDataSet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>you dont have to be crazy to work here serious...</td>\n",
       "      <td>[you, dont, have, to, be, crazy, to, work, her...</td>\n",
       "      <td>[you, dont, have, to, be, crazi, to, work, her...</td>\n",
       "      <td>[you, dont, have, to, be, crazy, to, work, her...</td>\n",
       "      <td>[dont, crazi, work, serious, illeg, requir, do...</td>\n",
       "      <td>[dont, crazi, work, serious, illeg, requir, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smh...im starting to fee...</td>\n",
       "      <td>Paw pawing my ass off smhim starting to feel b...</td>\n",
       "      <td>[Paw, pawing, my, ass, off, smhim, starting, t...</td>\n",
       "      <td>[paw, paw, my, ass, off, smhim, start, to, fee...</td>\n",
       "      <td>[Paw, pawing, my, as, off, smhim, starting, to...</td>\n",
       "      <td>[paw, paw, ass, smhim, start, feel, better, ]</td>\n",
       "      <td>[ass, start, feel, better]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>and ppl better not act like threatening suicid...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threatening...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threaten, s...</td>\n",
       "      <td>[and, ppl, better, not, act, like, threatening...</td>\n",
       "      <td>[ppl, better, act, like, threaten, suicid, res...</td>\n",
       "      <td>[ppl, better, act, like, threaten, suicid, res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@juliaroy you are just a tumbling fool. Love it.</td>\n",
       "      <td>you are just a tumbling fool. Love it.</td>\n",
       "      <td>juliaroy you are just a tumbling fool Love it</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbling, fool, ...</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbl, fool, lov...</td>\n",
       "      <td>[juliaroy, you, are, just, a, tumbling, fool, ...</td>\n",
       "      <td>[juliaroy, tumbl, fool, love, ]</td>\n",
       "      <td>[fool, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>Just gonna go shopping up Fosse Park with mate...</td>\n",
       "      <td>[Just, gonna, go, shopping, up, Fosse, Park, w...</td>\n",
       "      <td>[just, gonna, go, shop, up, foss, park, with, ...</td>\n",
       "      <td>[Just, gonna, go, shopping, up, Fosse, Park, w...</td>\n",
       "      <td>[gonna, go, shop, foss, park, mate, got, ï, ½7...</td>\n",
       "      <td>[gonna, go, shop, park, mate, got, ï]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                             Tweets  \\\n",
       "0          1  you dont have to be crazy to work here serious...   \n",
       "1          0  Paw pawing my ass off smh...im starting to fee...   \n",
       "2          1  and ppl better not act like threatening suicid...   \n",
       "3          0  @juliaroy you are just a tumbling fool. Love it.    \n",
       "4          0  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                        Tweet_No_URL  \\\n",
       "0  you dont have to be crazy to work here serious...   \n",
       "1  Paw pawing my ass off smh...im starting to fee...   \n",
       "2  and ppl better not act like threatening suicid...   \n",
       "3            you are just a tumbling fool. Love it.    \n",
       "4  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                       Tweet_No_Punc  \\\n",
       "0  you dont have to be crazy to work here serious...   \n",
       "1  Paw pawing my ass off smhim starting to feel b...   \n",
       "2  and ppl better not act like threatening suicid...   \n",
       "3     juliaroy you are just a tumbling fool Love it    \n",
       "4  Just gonna go shopping up Fosse Park with mate...   \n",
       "\n",
       "                                        Tweet_Tokens  \\\n",
       "0  [you, dont, have, to, be, crazy, to, work, her...   \n",
       "1  [Paw, pawing, my, ass, off, smhim, starting, t...   \n",
       "2  [and, ppl, better, not, act, like, threatening...   \n",
       "3  [juliaroy, you, are, just, a, tumbling, fool, ...   \n",
       "4  [Just, gonna, go, shopping, up, Fosse, Park, w...   \n",
       "\n",
       "                                    FirstDataSetStem  \\\n",
       "0  [you, dont, have, to, be, crazi, to, work, her...   \n",
       "1  [paw, paw, my, ass, off, smhim, start, to, fee...   \n",
       "2  [and, ppl, better, not, act, like, threaten, s...   \n",
       "3  [juliaroy, you, are, just, a, tumbl, fool, lov...   \n",
       "4  [just, gonna, go, shop, up, foss, park, with, ...   \n",
       "\n",
       "                                        FirstDataSet  \\\n",
       "0  [you, dont, have, to, be, crazy, to, work, her...   \n",
       "1  [Paw, pawing, my, as, off, smhim, starting, to...   \n",
       "2  [and, ppl, better, not, act, like, threatening...   \n",
       "3  [juliaroy, you, are, just, a, tumbling, fool, ...   \n",
       "4  [Just, gonna, go, shopping, up, Fosse, Park, w...   \n",
       "\n",
       "                                       SecondDataSet  \\\n",
       "0  [dont, crazi, work, serious, illeg, requir, do...   \n",
       "1      [paw, paw, ass, smhim, start, feel, better, ]   \n",
       "2  [ppl, better, act, like, threaten, suicid, res...   \n",
       "3                    [juliaroy, tumbl, fool, love, ]   \n",
       "4  [gonna, go, shop, foss, park, mate, got, ï, ½7...   \n",
       "\n",
       "                                        ThirdDataSet  \n",
       "0  [dont, crazi, work, serious, illeg, requir, do...  \n",
       "1                         [ass, start, feel, better]  \n",
       "2  [ppl, better, act, like, threaten, suicid, res...  \n",
       "3                                       [fool, love]  \n",
       "4              [gonna, go, shop, park, mate, got, ï]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list = list(itertools.chain.from_iterable(data['SecondDataSet']))\n",
    "\n",
    "fd = nltk.FreqDist(flat_list)\n",
    "\n",
    "word_toKeep = list(filter(lambda x:2000>x[1]>3,fd.items()))\n",
    "\n",
    "word_list_ToKeep = [item[0] for item in word_toKeep]\n",
    "\n",
    "def remove_lessFreq(tokens):\n",
    "    text_out = [word for word in tokens if word in word_list_ToKeep]\n",
    "    return text_out\n",
    "\n",
    "data['ThirdDataSet'] = data['SecondDataSet'].apply(lambda x:remove_lessFreq(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Words After Removing Less Frequency Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3293"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(itertools.chain.from_iterable(data['ThirdDataSet']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This vocabluary from data set\n",
    "UniqueWords = set(list(itertools.chain.from_iterable(data['ThirdDataSet'])))\n",
    "\n",
    "def join_token(token):\n",
    "    document = \" \".join([word for word in token if not word.isdigit()])\n",
    "    return document\n",
    "\n",
    "data['ThirdDataSetDocument'] = data[\"ThirdDataSet\"].apply(lambda x:join_token(x))\n",
    "\n",
    "#data['ThirdDataSetDocument'].head()\n",
    "#print(UniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1st  2nd  3rd  abil  abl  abort  abov  absenc  absolut  abt  ...  œyou  \\\n",
      "0       0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "1       0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "2       0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "3       0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "4       0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "...   ...  ...  ...   ...  ...    ...   ...     ...      ...  ...  ...   ...   \n",
      "9995    0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "9996    0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "9997    0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "9998    0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "9999    0    0    0     0    0      0     0       0        0    0  ...     0   \n",
      "\n",
      "      œyouâ  œã  œðÿ  šã  šðÿ  žã  žðÿ  ˆà  ˆã  \n",
      "0         0   0    0   0    0   0    0   0   0  \n",
      "1         0   0    0   0    0   0    0   0   0  \n",
      "2         0   0    0   0    0   0    0   0   0  \n",
      "3         0   0    0   0    0   0    0   0   0  \n",
      "4         0   0    0   0    0   0    0   0   0  \n",
      "...     ...  ..  ...  ..  ...  ..  ...  ..  ..  \n",
      "9995      0   0    0   0    0   0    0   0   0  \n",
      "9996      0   0    0   0    0   0    0   0   0  \n",
      "9997      0   0    0   0    0   0    0   0   0  \n",
      "9998      0   0    0   0    0   0    0   0   0  \n",
      "9999      0   0    0   0    0   0    0   0   0  \n",
      "\n",
      "[10000 rows x 3181 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "cv =CountVectorizer(UniqueWords)\n",
    "\n",
    "countvector = cv.fit_transform(data['ThirdDataSetDocument'])\n",
    "\n",
    "# CountVectorizer Data Frame\n",
    "\n",
    "countVectorDF = pd.DataFrame(countvector.toarray())\n",
    "countVectorDF.columns = cv.get_feature_names()\n",
    "\n",
    "print(countVectorDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = countVectorDF # Features\n",
    "y= data['Sentiment'] # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg = logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895714285714285"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9623333333333334"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluting the Result using Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1453,   38],\n",
       "       [  75, 1434]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cnf_mat = metrics.confusion_matrix(y_test,y_pred)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1491\n",
      "           1       0.97      0.95      0.96      1509\n",
      "\n",
      "    accuracy                           0.96      3000\n",
      "   macro avg       0.96      0.96      0.96      3000\n",
      "weighted avg       0.96      0.96      0.96      3000\n",
      "\n",
      "Accurracy: 0.9623333333333334\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accurracy:\",metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995714285714286"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1398,   93],\n",
       "       [  32, 1477]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_mat = metrics.confusion_matrix(y_test,y_pred)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1491\n",
      "           1       0.94      0.98      0.96      1509\n",
      "\n",
      "    accuracy                           0.96      3000\n",
      "   macro avg       0.96      0.96      0.96      3000\n",
      "weighted avg       0.96      0.96      0.96      3000\n",
      "\n",
      "Accurracy: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accurracy:\",metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Tweet_No_Punc']\n",
    "y = data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_linear = svm.SVC(kernel='linear')\n",
    "class_linear.fit(X_train_cv,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = class_linear.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954285714285714"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_linear.score(X_train_cv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9663333333333334"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_linear.score(X_test_cv,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Tweet_No_Punc']\n",
    "y = data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = cv.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train_cv,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnb.predict(X_test_cv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.944"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(X_train_cv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9223333333333333"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(X_test_cv,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1309,  148],\n",
       "       [  85, 1458]], dtype=int64)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_mat = metrics.confusion_matrix(y_test,y_pred)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      1457\n",
      "           1       0.91      0.94      0.93      1543\n",
      "\n",
      "    accuracy                           0.92      3000\n",
      "   macro avg       0.92      0.92      0.92      3000\n",
      "weighted avg       0.92      0.92      0.92      3000\n",
      "\n",
      "Accurracy: 0.9223333333333333\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accurracy:\",metrics.accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
